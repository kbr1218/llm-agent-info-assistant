# nodes.py
from app.tools import search, places
from langchain_core.messages import AIMessage
from app.agent.state import AgentState
from langchain.prompts import ChatPromptTemplate
from app.agent.model import llm
from app.agent.conditional_edge import conditional_from_search_prompt, conditional_from_search_parser
from app.functions import load_template_from_yaml, get_last_user_query, get_filtered_history
from langchain.output_parsers import StructuredOutputParser, ResponseSchema

response_template_with_context = load_template_from_yaml("prompt/respond_with_context.yaml")
response_template_without_context = load_template_from_yaml("prompt/respond_without_context.yaml")
refine_place_query_template = load_template_from_yaml("prompt/refine_place_query.yaml")
refine_search_query_template = load_template_from_yaml("prompt/refine_search_query.yaml")

response_prompt_with_context = ChatPromptTemplate.from_template(template=response_template_with_context)
response_prompt_without_context = ChatPromptTemplate.from_template(template=response_template_without_context)
refine_place_query_prompt = ChatPromptTemplate.from_template(template=refine_place_query_template)
refine_search_query_prompt = ChatPromptTemplate.from_template(template=refine_search_query_template)

### SerpAPIë¥¼ ìœ„í•œ ê²€ìƒ‰ì–´ ì „ì²˜ë¦¬ ë…¸ë“œ
def search_query_refiner_node(state):
    query = get_last_user_query(state["messages"])
    history = get_filtered_history(state["messages"], exclude_query=query)

    prompt = refine_search_query_prompt.format(
        query=query,
        history=history
    )
    refined_query = llm.invoke(prompt).content.strip()

    return {
        # "messages": [AIMessage(content=f"[ê²€ìƒ‰ìš© ë³´ì • ì¿¼ë¦¬]\n{refined_query}")],
        "refined_place_query": refined_query
    }

### ê²€ìƒ‰ ë…¸ë“œ í•¨ìˆ˜ ì •ì˜
# ë…¸ë“œëŠ” state["search_result"]ë¡œ ìƒíƒœë¥¼ ì½ê³  ìƒˆ ê°’ì„ ì¶”ê°€í•˜ë©´ ì´ë¥¼ ë³‘í•©í•´ì„œ ìƒíƒœë¥¼ ì´ì–´ê°
def search_node(state):
    query = state.get("refined_search_query", state["messages"][-1].content)

    # ë„êµ¬ ì‹¤í–‰ì˜ ê²°ê³¼ ì €ì¥
    result = search.run(query)
    return {
        # ë©€í‹°í„´ ëŒ€í™”ë¥¼ ìœ„í•œ ì‘ë‹µ ì €ì¥
        # toolsì—ì„œ ë‚˜ì˜¨ ê²°ê³¼ê°’ì€ ì—ì´ì „íŠ¸ì˜ ìµœì¢… ì‘ë‹µì— ê·¸ëŒ€ë¡œ ì¶œë ¥ë˜ë©´ ì•ˆ ë˜ì§€ë§Œ, ê·¸ë˜í”„ì˜ ë‚´ë¶€ ì¶”ë¡  íë¦„ì„ ìœ„í•œ context tracking ìš©ë„ë¡œ ì €ì¥í•¨
        # "messages": [AIMessage(content=result)],
        "search_result": result
    }

### Google Places APIë¥¼ ìœ„í•œ ê²€ìƒ‰ì–´ ì „ì²˜ë¦¬ ë…¸ë“œ
def place_query_refiner_node(state):
    query = get_last_user_query(state["messages"])
    search_result = state.get("search_result", "")      # search ê²½ë¡œê°€ ì•„ë‹ ê²½ìš° ë¹ˆ ë¬¸ìì—´
    history = get_filtered_history(state["messages"], exclude_query=query)

    prompt = refine_place_query_prompt.format(
        query=query,
        history=history,
        search_result=search_result
        )
    refined_query = llm.invoke(prompt).content.strip()

    return {
        # "messages": [AIMessage(content=f"[ì¥ì†Œ ê²€ìƒ‰ìš© ë³´ì • ì¿¼ë¦¬]\n{refined_query}")],
        "refined_place_query": refined_query
    }

### places ë…¸ë“œ í•¨ìˆ˜ ì •ì˜
def places_node(state):
    query = state.get("refined_place_query", state["messages"][-1].content)
    result = places.run(query)
    return {
        # "messages": [AIMessage(content=result)],
        "places_result": result
    }

### response ë…¸ë“œ í•¨ìˆ˜ ì •ì˜
# REVIEW: ëª…ì‹œì ìœ¼ë¡œ AgentState íƒ€ì…ì„ ì§€ì •í•œ ì´ìœ ? ì½”ë“œ ê°€ë…ì„± í–¥ìƒ, íƒ€ì… ê²€ì‚¬ ë„êµ¬
def response_node(state: AgentState):
    # ê²€ìƒ‰/ì£¼ì†Œ ê²°ê³¼
    search_result = state.get("search_result", "")
    places_result = state.get("places_result", "")
    
    query = get_last_user_query(state["messages"])
    history = get_filtered_history(state["messages"], exclude_query=query)

    # context êµ¬ì„±: ì •ë³´ + ë§¥ë½ í¬í•¨
    context = ""
    if places_result:
        context += f"[ì£¼ì†Œ ê²€ìƒ‰ ê²°ê³¼]\n{places_result}\n"
    if search_result:
        context += f"[ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½]\n{search_result}\n"

    # embed mapsë¥¼ ìœ„í•œ structured output schema ì •ì˜
    response_schema = [
        ResponseSchema(name="response_text", description="ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì¤„ ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€ì…ë‹ˆë‹¤."),
        ResponseSchema(name="map_place_id", description="Google Mapsì— í‘œì‹œí•  Google Place IDì…ë‹ˆë‹¤. í‘œì‹œí•  ê²Œ ì—†ë‹¤ë©´ ë¹ˆ ë¬¸ìì—´ë¡œ ì¶œë ¥í•˜ì„¸ìš”.")
    ]
    parser = StructuredOutputParser.from_response_schemas(response_schema)

    # ìµœì¢… ì‘ë‹µì„ ìƒì„±í•˜ê¸° ìœ„í•œ í”„ë¡¬í”„íŠ¸
    if context:
        prompt = response_prompt_with_context.format(
            query=query,
            context=context,
            history=history,
            format_instructions = parser.get_format_instructions()
        )
    else:
        prompt = response_prompt_without_context.format(
            query=query,
            history=history
        )
    raw_response = llm.invoke(prompt)
    parsed = parser.parse(raw_response.content)

    print("ğŸ” raw LLM response:", raw_response.content)
    print("âœ… parsed result:", parsed)

    return {
        "messages": [AIMessage(content=parsed["response_text"])],
        "map_place_id": parsed["map_place_id"] if parsed["map_place_id"] else None
    }

# INFO: user_input_nodeë¥¼ ì •ì˜í•˜ì§€ ì•Šì€ ì´ìœ ?
# í˜„ì¬ëŠ” streamlit (ë˜ëŠ” ì™¸ë¶€ ì…ë ¥)ì´ messageë¥¼ ì§ì ‘ ë„˜ê¸°ê¸° ë•Œë¬¸ì— ì‚¬ìš©ì ì…ë ¥ ë…¸ë“œë¥¼ ë”°ë¡œ ë‘˜ í•„ìš”ëŠ” ì—†ë‹¤. (+ ê°„ë‹¨í•œ êµ¬ì¡°ì´ê¸° ë•Œë¬¸)
# ë§Œì•½ ì‚¬ìš©ì ì…ë ¥ì„ ìƒíƒœì— ì¶”ê°€í•˜ê±°ë‚˜, ì „ì²˜ë¦¬(ì–¸ì–´ ê°ì§€, ìš•ì„¤ í•„í„° etc.)í•˜ëŠ” ë“± ê¸°ëŠ¥ì„ ì¶”ê°€í•œë‹¤ë©´ í•„ìš”ì— ë”°ë¼ ì •ì˜í•˜ë©´ ë¨

### conditional_function ì •ì˜
# LLMì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ì ì¿¼ë¦¬ì˜ ì˜ë„ë¥¼ ë¶„ë¥˜í•œ í›„ ê²€ìƒ‰ í›„ ì¥ì†Œë¥¼ ê²€ìƒ‰í• ì§€ ë§ì§€ ê²°ì •
def conditional_function_from_search_result(state):
    search_result = state.get("search_result", "")
    # ê°€ì¥ ìµœì‹  ì‚¬ìš©ì ì§ˆë¬¸ë§Œ ì¶”ì¶œ
    query = get_last_user_query((state["messages"]))
    # ê°€ì¥ ë§ˆì§€ë§‰ ë©”ì‹œì§€ ì œì™¸í•œ ëŒ€í™” ì´ë ¥
    history = get_filtered_history(state["messages"], exclude_query=query)

    # formated prompt ìƒì„± ë° LLM í˜¸ì¶œ
    prompt = conditional_from_search_prompt.format(
        query=query,
        history = history,
        search_result=search_result,
        format_instructions = conditional_from_search_parser.get_format_instructions()
    )
    response  = llm.invoke(prompt)
    parsed = conditional_from_search_parser.parse(response.content)
    return parsed["route"]
