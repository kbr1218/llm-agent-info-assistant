# nodes.py
from app.tools import search, places
from langchain_core.messages import AIMessage
from app.agent.state import AgentState
from langchain.prompts import ChatPromptTemplate
from langchain.output_parsers import StructuredOutputParser, ResponseSchema
from app.agent.model import llm
from app.agent.conditional_edge import conditional_from_search_prompt, conditional_from_search_parser
from app.functions import load_template_from_yaml, get_last_user_query, get_filtered_history


response_template_with_context = load_template_from_yaml("prompt/respond_with_context.yaml")
response_template_without_context = load_template_from_yaml("prompt/respond_without_context.yaml")
refine_place_query_template = load_template_from_yaml("prompt/refine_place_query.yaml")
refine_search_query_template = load_template_from_yaml("prompt/refine_search_query.yaml")

response_prompt_with_context = ChatPromptTemplate.from_template(template=response_template_with_context)
response_prompt_without_context = ChatPromptTemplate.from_template(template=response_template_without_context)
refine_place_query_prompt = ChatPromptTemplate.from_template(template=refine_place_query_template)
refine_search_query_prompt = ChatPromptTemplate.from_template(template=refine_search_query_template)

### SerpAPIë¥¼ ìœ„í•œ ê²€ìƒ‰ì–´ ì „ì²˜ë¦¬ ë…¸ë“œ
def search_query_refiner_node(state: AgentState) -> AgentState:
    query = get_last_user_query(state["messages"])
    history = get_filtered_history(state["messages"], exclude_query=query)
    retry_count = state.get("retry_count", 0)
    search_query = state.get("search_query", "")

    prompt = refine_search_query_prompt.format(
        query=query,
        history=history,
        retry_count = retry_count,
        search_query = search_query
    )
    refined_query = llm.invoke(prompt).content.strip()

    # ëˆ„ì ëœ search_query ì´ë ¥ ê°±ì‹ 
    updated_search_query = (
        f"{search_query}, {refined_query}" if search_query else refined_query
    )

    return {
        **state,
        "search_query": updated_search_query,
        "retry_count": retry_count     # ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ê³  response_nodeì—ì„œ ++
    }

### ê²€ìƒ‰ ë…¸ë“œ í•¨ìˆ˜ ì •ì˜
# ë…¸ë“œëŠ” state["search_result"]ë¡œ ìƒíƒœë¥¼ ì½ê³  ìƒˆ ê°’ì„ ì¶”ê°€í•˜ë©´ ì´ë¥¼ ë³‘í•©í•´ì„œ ìƒíƒœë¥¼ ì´ì–´ê°
def search_node(state):
    query = state.get("refined_search_query", state["messages"][-1].content)

    # ë„êµ¬ ì‹¤í–‰ì˜ ê²°ê³¼ ì €ì¥
    result = search.run(query)
    return {
        # ë©€í‹°í„´ ëŒ€í™”ë¥¼ ìœ„í•œ ì‘ë‹µ ì €ì¥
        # toolsì—ì„œ ë‚˜ì˜¨ ê²°ê³¼ê°’ì€ ì—ì´ì „íŠ¸ì˜ ìµœì¢… ì‘ë‹µì— ê·¸ëŒ€ë¡œ ì¶œë ¥ë˜ë©´ ì•ˆ ë˜ì§€ë§Œ, ê·¸ë˜í”„ì˜ ë‚´ë¶€ ì¶”ë¡  íë¦„ì„ ìœ„í•œ context tracking ìš©ë„ë¡œ ì €ì¥í•¨
        "search_result": result
    }

### Google Places APIë¥¼ ìœ„í•œ ê²€ìƒ‰ì–´ ì „ì²˜ë¦¬ ë…¸ë“œ
def place_query_refiner_node(state):
    query = get_last_user_query(state["messages"])
    search_result = state.get("search_result", "")      # search ê²½ë¡œê°€ ì•„ë‹ ê²½ìš° ë¹ˆ ë¬¸ìì—´
    history = get_filtered_history(state["messages"], exclude_query=query)

    prompt = refine_place_query_prompt.format(
        query=query,
        history=history,
        search_result=search_result
        )
    refined_query = llm.invoke(prompt).content.strip()

    return {
        # "messages": [AIMessage(content=f"[ì¥ì†Œ ê²€ìƒ‰ìš© ë³´ì • ì¿¼ë¦¬]\n{refined_query}")],
        "refined_place_query": refined_query
    }

### places ë…¸ë“œ í•¨ìˆ˜ ì •ì˜
def places_node(state):
    query = state.get("refined_place_query", state["messages"][-1].content)
    result = places.run(query)
    return {
        # "messages": [AIMessage(content=result)],
        "places_result": result
    }

### response ë…¸ë“œ í•¨ìˆ˜ ì •ì˜
# REVIEW: ëª…ì‹œì ìœ¼ë¡œ AgentState íƒ€ì…ì„ ì§€ì •í•œ ì´ìœ ? ì½”ë“œ ê°€ë…ì„± í–¥ìƒ, íƒ€ì… ê²€ì‚¬ ë„êµ¬
def response_node(state: AgentState):
    # ê²€ìƒ‰/ì£¼ì†Œ ê²°ê³¼
    search_result = state.get("search_result", "")
    places_result = state.get("places_result", "")
    
    query = get_last_user_query(state["messages"])
    history = get_filtered_history(state["messages"], exclude_query=query)

    # context êµ¬ì„±: ì •ë³´ + ë§¥ë½ í¬í•¨
    context = ""
    if places_result:
        context += f"[ì£¼ì†Œ ê²€ìƒ‰ ê²°ê³¼]\n{places_result}\n"
    if search_result:
        context += f"[ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½]\n{search_result}\n"

    # embed mapsë¥¼ ìœ„í•œ structured output schema ì •ì˜
    response_schema = [
        ResponseSchema(name="response_text", description="ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì¤„ ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€ì…ë‹ˆë‹¤."),
        ResponseSchema(name="map_place_id", description="Google Mapsì— í‘œì‹œí•  Google Place IDì…ë‹ˆë‹¤. í‘œì‹œí•  ê²Œ ì—†ë‹¤ë©´ ë¹ˆ ë¬¸ìì—´ë¡œ ì¶œë ¥í•˜ì„¸ìš”."),
        ResponseSchema(name="requery", description="ê²€ìƒ‰ ê²°ê³¼ê°€ ë¶€ì¡±í•˜ë‹¤ê³  íŒë‹¨ë˜ë©´ true, ì•„ë‹ˆë©´ false")
    ]
    parser = StructuredOutputParser.from_response_schemas(response_schema)

    # ìµœì¢… ì‘ë‹µì„ ìƒì„±í•˜ê¸° ìœ„í•œ í”„ë¡¬í”„íŠ¸
    if context:
        prompt = response_prompt_with_context.format(
            query=query,
            context=context,
            history=history,
            format_instructions = parser.get_format_instructions()
        )
    else:
        prompt = response_prompt_without_context.format(
            query=query,
            history=history,
            format_instructions=parser.get_format_instructions()
        )
    raw_response = llm.invoke(prompt)
    parsed = parser.parse(raw_response.content)

    print("ğŸ” raw LLM response:", raw_response.content)
    print("âœ… parsed result:", parsed)

    # retry_count ì¡°ê±´ë¶€ ì¦ê°€
    retry_count = state.get("retry_count", 0)
    requery = parsed["requery"]

    if requery:
        retry_count += 1

    # ìµœëŒ€ 3íšŒê¹Œì§€ë§Œ ì¬ê²€ìƒ‰ í—ˆìš©
    if retry_count > 3:
        requery = False

    return {
        "messages": [AIMessage(content=parsed["response_text"])],
        "map_place_id": parsed["map_place_id"] if parsed["map_place_id"] else None,
        "retry_count": retry_count,
        "requery": requery
    }

# INFO: user_input_nodeë¥¼ ì •ì˜í•˜ì§€ ì•Šì€ ì´ìœ ?
# í˜„ì¬ëŠ” streamlit (ë˜ëŠ” ì™¸ë¶€ ì…ë ¥)ì´ messageë¥¼ ì§ì ‘ ë„˜ê¸°ê¸° ë•Œë¬¸ì— ì‚¬ìš©ì ì…ë ¥ ë…¸ë“œë¥¼ ë”°ë¡œ ë‘˜ í•„ìš”ëŠ” ì—†ë‹¤. (+ ê°„ë‹¨í•œ êµ¬ì¡°ì´ê¸° ë•Œë¬¸)
# ë§Œì•½ ì‚¬ìš©ì ì…ë ¥ì„ ìƒíƒœì— ì¶”ê°€í•˜ê±°ë‚˜, ì „ì²˜ë¦¬(ì–¸ì–´ ê°ì§€, ìš•ì„¤ í•„í„° etc.)í•˜ëŠ” ë“± ê¸°ëŠ¥ì„ ì¶”ê°€í•œë‹¤ë©´ í•„ìš”ì— ë”°ë¼ ì •ì˜í•˜ë©´ ë¨

### conditional_function ì •ì˜
# LLMì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ì ì¿¼ë¦¬ì˜ ì˜ë„ë¥¼ ë¶„ë¥˜í•œ í›„ ê²€ìƒ‰ í›„ ì¥ì†Œë¥¼ ê²€ìƒ‰í• ì§€ ë§ì§€ ê²°ì •
def conditional_function_from_search_result(state):
    search_result = state.get("search_result", "")
    # ê°€ì¥ ìµœì‹  ì‚¬ìš©ì ì§ˆë¬¸ë§Œ ì¶”ì¶œ
    query = get_last_user_query((state["messages"]))
    # ê°€ì¥ ë§ˆì§€ë§‰ ë©”ì‹œì§€ ì œì™¸í•œ ëŒ€í™” ì´ë ¥
    history = get_filtered_history(state["messages"], exclude_query=query)

    # formated prompt ìƒì„± ë° LLM í˜¸ì¶œ
    prompt = conditional_from_search_prompt.format(
        query=query,
        history = history,
        search_result=search_result,
        format_instructions = conditional_from_search_parser.get_format_instructions()
    )
    response  = llm.invoke(prompt)
    parsed = conditional_from_search_parser.parse(response.content)
    return parsed["route"]

# ì¬ê²€ìƒ‰ì„ ìœ„í•œ ì¡°ê±´ë¶€ í•¨ìˆ˜ ì •ì˜
def requery_router(state: AgentState) -> str:
    retry_count = state.get("retry_count", 0)
    requery = state.get("requery", False)

    if retry_count > 3:
        return "end"
    if requery:
        return "search_query_refiner"
    return "end"